{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c79e5c1",
   "metadata": {},
   "source": [
    "# Azure assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef430d5d",
   "metadata": {},
   "source": [
    "The datasets that were used in this notebook can be found below:\n",
    "    \n",
    "Main dataset: https://www.kaggle.com/datasets/unsdsn/world-happiness</br>\n",
    "Secondary dataset: https://www.kaggle.com/datasets/theworldbank/education-statistics\n",
    "\n",
    "The main dataset consists of the world's happiness score which is determined based on various factors. As a secondary table, I chose to pick a column from the education statistics dataset which is 'Income Group'. I wanted whether the education level leading to the income group has any significant relationship when it comes to the world's happiness score per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    print('NumPy already installed, only imported')\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "    print('NumPy was not installed, installed and imported')\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print('pandas already installed, only imported')\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    print('pandas was not installed, installed and imported')\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print('sklearn already installed, only imported')\n",
    "except:\n",
    "    !pip install scikit-learn\n",
    "    import sklearn\n",
    "    print('sklearn was not installed, installed and imported')\n",
    "\n",
    "try: \n",
    "    import time    \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.svm import SVR\n",
    "    print('All of the remaning libraries have been imported')\n",
    "except: \n",
    "    print(\"Not all libraries have been imported correctly, please check again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "10ecb0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(happiness_DF, education_DF, new_data=None):\n",
    "    #change names of the columns in both datasets so that it can be prepared for merging\n",
    "    happiness_DF = happiness_DF.rename(columns={'Country or region': 'Country'})\n",
    "    education_DF = education_DF.rename(columns={'Table Name': 'Country', 'Income Group': 'Income_Group'})\n",
    "    \n",
    "    #merge datasets based on the country (only merge income_group column which is our new feature from another dataset)\n",
    "    merged_DF = pd.merge(happiness_DF, education_DF[['Country', 'Income_Group']], on='Country', how='left')\n",
    "    \n",
    "    #remove unnecessary columns\n",
    "    merged_DF = merged_DF.drop(['Overall rank', 'Country'], axis=1)\n",
    "    \n",
    "    #check for missing values, if any, remove\n",
    "    print(f\"There are {merged_DF.isna().sum().sum()} missing values in your dataset\")\n",
    "    merged_DF = merged_DF.dropna()\n",
    "    print(f\"There are {merged_DF.isna().sum().sum()} missing left in the dataset\")\n",
    "    \n",
    "    #if new data is not none (if it is picked) then merge it with the dataframe\n",
    "    if new_data is not None:\n",
    "        merged_DF = pd.concat([merged_DF, new_data], ignore_index=True)\n",
    "    \n",
    "    # make temporary dataframe that holds numerical values for categorical column (this is used for determine top features)\n",
    "    encoded_DF = pd.get_dummies(merged_DF, columns=['Income_Group'])\n",
    "    \n",
    "    print(\"\\nTop 3 Features using mutual information scores\")\n",
    "    X, y = encoded_DF.drop('Score', axis=1), merged_DF['Score']\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    print(mi_scores.head(3))\n",
    "    \n",
    "    print(\"\\nTop 3 Features using feature importance\")\n",
    "    X, y = encoded_DF.drop('Score', axis=1), merged_DF['Score']\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X, y)\n",
    "    importances = model.feature_importances_\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    sorted_importances = importances[sorted_indices]\n",
    "    sorted_feature_names = X.columns[sorted_indices]\n",
    "    print(sorted_feature_names[:3])\n",
    "    \n",
    "    print(\"\\nTop 3 Features using correlation matrix\")\n",
    "    correlation_matrix = merged_DF.corr(numeric_only=True)\n",
    "    correlation_values = correlation_matrix['Score'].abs()\n",
    "    sorted_correlation_values = correlation_values.sort_values(ascending=False)\n",
    "    print(sorted_correlation_values.index[1:4])\n",
    "    \n",
    "    #create a numeric transformer to pre-process numerical features using an imputer and a scaler\n",
    "    numeric_features = [\"GDP per capita\", \"Social support\", \"Healthy life expectancy\", \"Freedom to make life choices\"]\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    #create a categorical transformer to pre-process categorical features using an imputer and an encoder\n",
    "    categorical_features = [\"Income_Group\"]\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    # add the transformer to the pre-processor variable\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numerical', numeric_transformer, numeric_features),\n",
    "            ('categorical', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    # create a pipeline and append the Support Vector Regressor classifier\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', SVR())])\n",
    "    \n",
    "    #split data into X (independent) and y (dependent)\n",
    "    X = merged_DF[[\"GDP per capita\", \"Social support\", \"Healthy life expectancy\", \"Freedom to make life choices\", \"Income_Group\"]]\n",
    "    y = merged_DF['Score']\n",
    "    \n",
    "    #split data into train and test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "    \n",
    "    #train the model of the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    #evaluate the model's performance\n",
    "    print(\"\\nEvalution of the score/accuracy:\")\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print(f'The R-Squared score is: {score}')\n",
    "    print(\"Accuracy: %.2f%%\" % (score * 100))\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error: \", mse)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e98e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 missing values in your dataset\n",
      "There are 0 missing left in the dataset\n",
      "\n",
      "Top 3 Features using mutual information scores\n",
      "Healthy life expectancy    0.609765\n",
      "Social support             0.602643\n",
      "GDP per capita             0.597983\n",
      "dtype: float64\n",
      "\n",
      "Top 3 Features using feature importance\n",
      "Index(['Social support', 'Healthy life expectancy', 'GDP per capita'], dtype='object')\n",
      "\n",
      "Top 3 Features using correlation matrix\n",
      "Index(['GDP per capita', 'Healthy life expectancy', 'Social support'], dtype='object')\n",
      "\n",
      "Evalution of the score/accuracy:\n",
      "The R-Squared score is: 0.7859008720392056\n",
      "Accuracy: 78.59%\n",
      "Mean Squared Error:  0.19671829859073958\n",
      "Mean Absolute Error: 0.3306771564431313\n",
      "\n",
      "Execution time of the pipeline is: 0.24153804779052734 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "happiness_DF = pd.read_csv('world-happiness-2019.csv')\n",
    "education_DF = pd.read_csv('education-statistics.csv')\n",
    "\n",
    "execute_pipeline(happiness_DF, education_DF)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"\\nExecution time of the pipeline is:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b45d265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new data to fit into the pipeline\n",
    "new_data = pd.DataFrame([\n",
    "        {'Score': 7.142, 'GDP per capita': 1.125, 'Social support': 1.421, \n",
    "        'Healthy life expectancy': 0.898, \n",
    "        'Freedom to make life choices': 0.463, 'Generosity': 0.319, \n",
    "        'Perceptions of corruption': 0.512, 'Income_Group': 'High income: nonOECD'\n",
    "        },\n",
    "        {'Score': 6.322, 'GDP per capita': 1.315, 'Social support': 1.350, \n",
    "        'Healthy life expectancy': 0.923, \n",
    "        'Freedom to make life choices': 0.341, 'Generosity': 0.521, \n",
    "        'Perceptions of corruption': 0.456, 'Income_Group': 'Upper middle income'\n",
    "        },\n",
    "        {'Score': 5.331, 'GDP per capita': 1.032, 'Social support': 1.231, \n",
    "        'Healthy life expectancy': 1.002, \n",
    "        'Freedom to make life choices': 0.521, 'Generosity': 0.299, \n",
    "        'Perceptions of corruption': 0.441, 'Income_Group': 'Low income'\n",
    "        }\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a0c92ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 missing values in your dataset\n",
      "There are 0 missing left in the dataset\n",
      "\n",
      "Top 3 Features using mutual information scores\n",
      "Social support             0.596968\n",
      "Healthy life expectancy    0.594566\n",
      "GDP per capita             0.588017\n",
      "dtype: float64\n",
      "\n",
      "Top 3 Features using feature importance\n",
      "Index(['Social support', 'GDP per capita', 'Healthy life expectancy'], dtype='object')\n",
      "\n",
      "Top 3 Features using correlation matrix\n",
      "Index(['GDP per capita', 'Healthy life expectancy', 'Social support'], dtype='object')\n",
      "\n",
      "Evalution of the score/accuracy:\n",
      "The R-Squared score is: 0.6064207536243638\n",
      "Accuracy: 60.64%\n",
      "Mean Squared Error:  0.35449261029003865\n",
      "Mean Absolute Error: 0.47669080563396715\n",
      "\n",
      "Execution time of the pipeline is: 0.23754668235778809 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#execute the pipeness with the addition of new data\n",
    "execute_pipeline(happiness_DF, education_DF, new_data)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"\\nExecution time of the pipeline is:\", execution_time, \"seconds\")\n",
    "\n",
    "#set new data variable back to none so that it does not get re-used when method is executed\n",
    "new_data = None "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
